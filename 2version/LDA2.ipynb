{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jtDbXLDYEQ9HtNTnWspa5bFH7HCPPDsw","authorship_tag":"ABX9TyP+kUjRNwILVrO/u6gOx19K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#pip install --upgrade ipykernel"],"metadata":{"id":"rRljTF7WVh2t","executionInfo":{"status":"ok","timestamp":1683466553736,"user_tz":-180,"elapsed":4,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#!pip install pyLDAvis"],"metadata":{"id":"l2rITEPPR9f1","executionInfo":{"status":"ok","timestamp":1683466642538,"user_tz":-180,"elapsed":720,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import gensim\n","import pyLDAvis\n","import pyLDAvis.gensim\n","import spacy\n","import pandas as pd\n","import nltk; nltk.download('stopwords')\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","import re\n","import warnings\n","from pprint import pprint\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import seaborn as sns\n","%config InlineBackend.figure_formats = ['retina']\n","from sklearn.metrics import f1_score\n","from sklearn import linear_model\n","from sklearn import metrics\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import fbeta_score\n","import matplotlib.pyplot as plt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zp0Zt3N_R3sP","executionInfo":{"status":"ok","timestamp":1683478357340,"user_tz":-180,"elapsed":240,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}},"outputId":"7a9566c8-1ee1-483e-f35c-91b8a0f3fb6b"},"execution_count":152,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"xYy_43TXeNgc"}},{"cell_type":"code","source":["with open('drive/MyDrive/Diplom/test2.csv') as f:\n","    test = pd.read_csv(f,sep='\\t')\n","with open('drive/MyDrive/Diplom/train2.csv') as f:\n","    train = pd.read_csv(f,sep='\\t')"],"metadata":{"id":"_W5dLeyUU7if","executionInfo":{"status":"ok","timestamp":1683478385089,"user_tz":-180,"elapsed":2905,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":153,"outputs":[]},{"cell_type":"code","source":["def bootstrap(data, freq):\n","    freq = freq.set_index('Y')\n","\n","    def sampleClass(classgroup):\n","        cls = classgroup['Y'].iloc[0]\n","        nDesired = freq.nostoextract[cls]\n","        nRows = len(classgroup)\n","\n","        nSamples = min(nRows, nDesired)\n","        return classgroup.sample(nSamples)\n","\n","    samples = data.groupby('Y').apply(sampleClass)\n","\n","    samples.index = range(len(samples))\n","\n","    return samples\n","\n","freq = pd.DataFrame({'Y':[0, 1, 2],\n","                     'nostoextract':[6552, 6552, 6552]})\n","\n","train = bootstrap(train, freq)"],"metadata":{"id":"ctMbPps9c28y","executionInfo":{"status":"ok","timestamp":1683478385090,"user_tz":-180,"elapsed":10,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":["Prep Review Text for LDA - Need to make Bigram Model"],"metadata":{"id":"KWox55Bqbr4_"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['get','say'])\n","#stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n","#                  'would','really','like','great','service','came','got'])"],"metadata":{"id":"jOAVBaboWS-b","executionInfo":{"status":"ok","timestamp":1683478385091,"user_tz":-180,"elapsed":8,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":155,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"4V3jloJLWU2L","executionInfo":{"status":"ok","timestamp":1683478386608,"user_tz":-180,"elapsed":1524,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":156,"outputs":[]},{"cell_type":"code","source":["def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n","        \n","def remove_stopwords(texts):\n","    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"],"metadata":{"id":"R8ImSjGBWXWa","executionInfo":{"status":"ok","timestamp":1683478386609,"user_tz":-180,"elapsed":12,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":157,"outputs":[]},{"cell_type":"code","source":["# def lemmatization(texts, allowed_postags=['NOUN','ADV']):\n","#     texts_out = []\n","#     for sent in texts:\n","#         doc = nlp(\" \".join(sent)) \n","#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","#     return texts_out"],"metadata":{"id":"H8UaD9ZkXbZt","executionInfo":{"status":"ok","timestamp":1683478386610,"user_tz":-180,"elapsed":11,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":158,"outputs":[]},{"cell_type":"code","source":["def bigrams(words, bi_min=5, tri_min=5):\n","    bigram = gensim.models.Phrases(words, min_count = bi_min)\n","    bigram_mod = gensim.models.phrases.Phraser(bigram)\n","    return bigram_mod"],"metadata":{"id":"ZfBnN_EXWfiq","executionInfo":{"status":"ok","timestamp":1683478386611,"user_tz":-180,"elapsed":11,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":159,"outputs":[]},{"cell_type":"code","source":["def get_corpus(df):\n","    \"\"\"\n","    Get Bigram Model, Corpus, id2word mapping\n","    \"\"\"\n","    \n","#   df['body'] = strip_newline(df.body)\n","    words = list(sent_to_words(df.body))\n","    words = remove_stopwords(words)\n","    bigram = bigrams(words)\n","    bigram = [bigram[review] for review in words]\n","#     lemma = lemmatization(bigram)\n","    id2word = gensim.corpora.Dictionary(bigram)\n","    id2word.filter_extremes(no_below=10, no_above=0.35)\n","    id2word.compactify()\n","    corpus = [id2word.doc2bow(body) for body in bigram]\n","    return corpus, id2word, bigram"],"metadata":{"id":"C2l0KSBZWhkt","executionInfo":{"status":"ok","timestamp":1683478386612,"user_tz":-180,"elapsed":11,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":160,"outputs":[]},{"cell_type":"code","source":["train_corpus, train_id2word, bigram_train = get_corpus(train)"],"metadata":{"id":"BROJgLKkWkG3","executionInfo":{"status":"ok","timestamp":1683478391143,"user_tz":-180,"elapsed":4541,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":161,"outputs":[]},{"cell_type":"code","source":["bigram_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0ZvJZ3bYPg5","executionInfo":{"status":"ok","timestamp":1683478391145,"user_tz":-180,"elapsed":12,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}},"outputId":"2cdd4df2-7f7c-4520-c82b-4597bebbae7c"},"execution_count":162,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['http_www', 'liveleak', 'com', 'view', 'c_']"]},"metadata":{},"execution_count":162}]},{"cell_type":"markdown","source":["# LDA Model in Gensim 20 topics"],"metadata":{"id":"jntM2MXAbxo6"}},{"cell_type":"code","source":["TOPICS = 20"],"metadata":{"id":"CkAggL2pZZoF","executionInfo":{"status":"ok","timestamp":1683478391146,"user_tz":-180,"elapsed":9,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":163,"outputs":[]},{"cell_type":"code","source":["import logging\n","logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter('ignore')\n","    lda_train = gensim.models.ldamulticore.LdaMulticore(\n","                           corpus=train_corpus,\n","                           num_topics=TOPICS,\n","                           id2word=train_id2word,\n","                           chunksize=100,\n","                           workers=7, # Num. Processing Cores - 1\n","                           passes=50,\n","                           eval_every = 1,\n","                           per_word_topics=True)\n","    lda_train.save('lda_train.model')"],"metadata":{"id":"YZx-aPiHZB2-","executionInfo":{"status":"ok","timestamp":1683478575340,"user_tz":-180,"elapsed":184201,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":164,"outputs":[]},{"cell_type":"markdown","source":["Example of Topics"],"metadata":{"id":"8YM2GPgbb2VR"}},{"cell_type":"code","source":["lda_train.print_topics(3,num_words=15)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CxJ9Jr2ZN1P","executionInfo":{"status":"ok","timestamp":1683478575341,"user_tz":-180,"elapsed":22,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}},"outputId":"a019fce8-e8f4-4c30-b37a-74a1c494bed2"},"execution_count":165,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(19,\n","  '0.197*\"com\" + 0.088*\"http_imgur\" + 0.035*\"point\" + 0.034*\"png\" + 0.029*\"jpg\" + 0.029*\"http\" + 0.029*\"https\" + 0.028*\"http_www\" + 0.024*\"exactly\" + 0.018*\"interesting\" + 0.018*\"imgur_com\" + 0.014*\"needs\" + 0.013*\"https_github\" + 0.012*\"comments\" + 0.011*\"sounds\"'),\n"," (10,\n","  '0.087*\"think\" + 0.026*\"like\" + 0.026*\"time\" + 0.025*\"work\" + 0.025*\"though\" + 0.024*\"could\" + 0.023*\"make\" + 0.019*\"life\" + 0.019*\"would\" + 0.018*\"oh\" + 0.017*\"least\" + 0.017*\"kind\" + 0.016*\"sorry\" + 0.016*\"without\" + 0.015*\"one\"'),\n"," (14,\n","  '0.140*\"know\" + 0.075*\"yeah\" + 0.045*\"look\" + 0.044*\"actually\" + 0.030*\"problem\" + 0.025*\"name\" + 0.025*\"call\" + 0.024*\"never\" + 0.019*\"show\" + 0.017*\"played\" + 0.014*\"cops\" + 0.013*\"http_www\" + 0.013*\"hold\" + 0.012*\"reading\" + 0.012*\"dropbox_com\"')]"]},"metadata":{},"execution_count":165}]},{"cell_type":"markdown","source":["Make Vectors"],"metadata":{"id":"woyPzhoBb5lu"}},{"cell_type":"markdown","source":["Now that we have an LDA model, we need to run all the reviews through it using 'get document topics'. A list comprehension on that output (2nd line in loop) will give the probability distribution of the topics for a specific review, and that's our feature vector. The other 2 lines in the loop are hand-engineered features."],"metadata":{"id":"8ZA7dqsIb_wR"}},{"cell_type":"code","source":["train_vecs = []\n","for i in range(len(train)):\n","    top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n","    topic_vec = [top_topics[i][1] for i in range(TOPICS)]\n","    train_vecs.append(topic_vec)"],"metadata":{"id":"TQNJ50e_ZVLO","executionInfo":{"status":"ok","timestamp":1683478579420,"user_tz":-180,"elapsed":4097,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":166,"outputs":[]},{"cell_type":"code","source":["X20 = np.array(train_vecs)\n","y20 = np.array(train.Y)"],"metadata":{"id":"F-azVJF9cFJv","executionInfo":{"status":"ok","timestamp":1683478579421,"user_tz":-180,"elapsed":20,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":167,"outputs":[]},{"cell_type":"markdown","source":["Final 5-fold CV loop for training on 2016 review data"],"metadata":{"id":"-ZHE606tcaGE"}},{"cell_type":"code","source":["kf = KFold(5, shuffle=True, random_state=42)\n","cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n","\n","for train_ind, val_ind in kf.split(X20, y20):\n","    # Assign CV IDX\n","    X_train, y_train = X20[train_ind], y20[train_ind]\n","    X_val, y_val = X20[val_ind], y20[val_ind]\n","    \n","    # Scale Data\n","    scaler = StandardScaler()\n","    X_train_scale = scaler.fit_transform(X_train)\n","    X_val_scale = scaler.transform(X_val)\n","\n","    # Logisitic Regression\n","    lr = LogisticRegression(\n","        class_weight= 'balanced',\n","        solver='newton-cg',\n","        fit_intercept=True\n","    ).fit(X_train_scale, y_train)\n","\n","    y_pred = lr.predict(X_val_scale)\n","    cv_lr_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","    \n","    # Logistic Regression Mini-Batch SGD\n","    sgd = linear_model.SGDClassifier(\n","        max_iter=1000,\n","        tol=1e-3,\n","        loss='log_loss',\n","        class_weight='balanced'\n","    ).fit(X_train_scale, y_train)\n","    \n","    y_pred = sgd.predict(X_val_scale)\n","    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","    \n","    # SGD Modified Huber\n","    sgd_huber = linear_model.SGDClassifier(\n","        max_iter=1000,\n","        tol=1e-3,\n","        alpha=20,\n","        loss='modified_huber',\n","        class_weight='balanced'\n","    ).fit(X_train_scale, y_train)\n","    \n","    y_pred = sgd_huber.predict(X_val_scale)\n","    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","\n","print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n","print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n","print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Joi-qgOBcaz-","executionInfo":{"status":"ok","timestamp":1683478586377,"user_tz":-180,"elapsed":6974,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}},"outputId":"f9712eb1-0efa-42d0-cb03-878cfcc73cdc"},"execution_count":168,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression Val f1: 0.476 +- 0.007\n","Logisitic Regression SGD Val f1: 0.464 +- 0.014\n","SVM Huber Val f1: 0.310 +- 0.122\n"]}]},{"cell_type":"markdown","source":["# LDA Model in Gensim 3 topics"],"metadata":{"id":"JXfx5ZrEctU_"}},{"cell_type":"code","source":["TOPICS = 3"],"metadata":{"id":"p_AEzOTLcvYL","executionInfo":{"status":"ok","timestamp":1683478586377,"user_tz":-180,"elapsed":20,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":169,"outputs":[]},{"cell_type":"code","source":["import logging\n","logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter('ignore')\n","    lda_train = gensim.models.ldamulticore.LdaMulticore(\n","                           corpus=train_corpus,\n","                           num_topics=TOPICS,\n","                           id2word=train_id2word,\n","                           chunksize=100,\n","                           workers=7, # Num. Processing Cores - 1\n","                           passes=50,\n","                           eval_every = 1,\n","                           per_word_topics=True)\n","    lda_train.save('lda_train.model')"],"metadata":{"id":"Fc_zgdQ6c1i4","executionInfo":{"status":"ok","timestamp":1683478745296,"user_tz":-180,"elapsed":158938,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":170,"outputs":[]},{"cell_type":"code","source":["train_vecs = []\n","for i in range(len(train)):\n","    top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n","    topic_vec = [top_topics[i][1] for i in range(TOPICS)]\n","    train_vecs.append(topic_vec)"],"metadata":{"id":"7Ran_bdec6V_","executionInfo":{"status":"ok","timestamp":1683478747457,"user_tz":-180,"elapsed":2182,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":171,"outputs":[]},{"cell_type":"code","source":["X3 = np.array(train_vecs)\n","y3 = np.array(train.Y)"],"metadata":{"id":"RnpHV9f8c9Ur","executionInfo":{"status":"ok","timestamp":1683478747459,"user_tz":-180,"elapsed":6,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":172,"outputs":[]},{"cell_type":"code","source":["kf = KFold(5, shuffle=True, random_state=42)\n","cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n","\n","for train_ind, val_ind in kf.split(X3, y3):\n","    # Assign CV IDX\n","    X_train, y_train = X3[train_ind], y3[train_ind]\n","    X_val, y_val = X3[val_ind], y3[val_ind]\n","    \n","    # Scale Data\n","    scaler = StandardScaler()\n","    X_train_scale = scaler.fit_transform(X_train)\n","    X_val_scale = scaler.transform(X_val)\n","\n","    # Logisitic Regression\n","    lr = LogisticRegression(\n","        class_weight= 'balanced',\n","        solver='newton-cg',\n","        fit_intercept=True\n","    ).fit(X_train_scale, y_train)\n","\n","    y_pred = lr.predict(X_val_scale)\n","    cv_lr_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","    \n","    # Logistic Regression Mini-Batch SGD\n","    sgd = linear_model.SGDClassifier(\n","        max_iter=1000,\n","        tol=1e-3,\n","        loss='log_loss',\n","        class_weight='balanced'\n","    ).fit(X_train_scale, y_train)\n","    \n","    y_pred = sgd.predict(X_val_scale)\n","    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","    \n","    # SGD Modified Huber\n","    sgd_huber = linear_model.SGDClassifier(\n","        max_iter=1000,\n","        tol=1e-3,\n","        alpha=20,\n","        loss='modified_huber',\n","        class_weight='balanced'\n","    ).fit(X_train_scale, y_train)\n","    \n","    y_pred = sgd_huber.predict(X_val_scale)\n","    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","\n","print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n","print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n","print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXFQTanidBzZ","executionInfo":{"status":"ok","timestamp":1683478749608,"user_tz":-180,"elapsed":2154,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}},"outputId":"0ae199e0-2205-4c5d-b536-2cdbb806cb7d"},"execution_count":173,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression Val f1: 0.527 +- 0.008\n","Logisitic Regression SGD Val f1: 0.503 +- 0.039\n","SVM Huber Val f1: 0.272 +- 0.177\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n"]}]},{"cell_type":"markdown","source":["# 20 and 3"],"metadata":{"id":"DHAOMmxYeGHx"}},{"cell_type":"code","source":["X = np.concatenate((X3,X20), axis=1)\n","y = y3"],"metadata":{"id":"dLew0pb3hrMU","executionInfo":{"status":"ok","timestamp":1683478749609,"user_tz":-180,"elapsed":6,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}}},"execution_count":174,"outputs":[]},{"cell_type":"code","source":["kf = KFold(5, shuffle=True, random_state=42)\n","cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n","\n","for train_ind, val_ind in kf.split(X, y):\n","    # Assign CV IDX\n","    X_train, y_train = X[train_ind], y[train_ind]\n","    X_val, y_val = X[val_ind], y[val_ind]\n","    \n","    # Scale Data\n","    scaler = StandardScaler()\n","    X_train_scale = scaler.fit_transform(X_train)\n","    X_val_scale = scaler.transform(X_val)\n","\n","    # Logisitic Regression\n","    lr = LogisticRegression(\n","        class_weight= 'balanced',\n","        solver='newton-cg',\n","        fit_intercept=True\n","    ).fit(X_train_scale, y_train)\n","\n","    y_pred = lr.predict(X_val_scale)\n","    cv_lr_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","    \n","    # Logistic Regression Mini-Batch SGD\n","    sgd = linear_model.SGDClassifier(\n","        max_iter=1000,\n","        tol=1e-3,\n","        loss='log_loss',\n","        class_weight='balanced'\n","    ).fit(X_train_scale, y_train)\n","    \n","    y_pred = sgd.predict(X_val_scale)\n","    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","    \n","    # SGD Modified Huber\n","    sgd_huber = linear_model.SGDClassifier(\n","        max_iter=1000,\n","        tol=1e-3,\n","        alpha=20,\n","        loss='modified_huber',\n","        class_weight='balanced'\n","    ).fit(X_train_scale, y_train)\n","    \n","    y_pred = sgd_huber.predict(X_val_scale)\n","    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='weighted'))\n","\n","print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n","print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n","print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKAs3q1ofGPl","executionInfo":{"status":"ok","timestamp":1683478754257,"user_tz":-180,"elapsed":4652,"user":{"displayName":"Анна Ермакова","userId":"06588401722128011033"}},"outputId":"a6f40bb7-1eb6-4aed-8199-53792eb70086"},"execution_count":175,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n","/usr/local/lib/python3.10/dist-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n","  warn('The line search algorithm did not converge', LineSearchWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n","  warnings.warn(\"Line Search failed\")\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression Val f1: 0.575 +- 0.007\n","Logisitic Regression SGD Val f1: 0.572 +- 0.016\n","SVM Huber Val f1: 0.326 +- 0.153\n"]}]}]}